Report Project 2
Group members: jowid100, moy75pic, flkar101, kow37neq

For data we decided to put all the data in one folder, because it would make it easier to work on later on. The data_loader and sampler for the first model dynamically gets the files from the data folder that means each of us could have different sections downloaded in our data file. It also normalizes the data between 0 and 1 and pads the patch if it is too small.

We decided to use the data with size “06” for training as this was the coarsest resolution that was still possible to use with patch_size = 64.  For the first model we decided to try out a GAN. We first started with a model with only fully connected layers but moved on very quickly as this only produced random noise. We added a convolutional layer on top. This already worked a lot better than the fully connected one, but it was still not similar to the training data at all. We tried some different variations of layer architecture to make the Generator more powerful, but this didn’t work so well. We then tried some different Learning rates to make the Discriminator perform worse which balanced out the loss a bit, but the Discriminator was still way too powerful. We then added some random noise to the real and fake images to make the decision harder for the Discriminator which helped the most. To further stabilize the training and the loss we added label smoothing. After all this we already got results that showed that the Generator is learning something useful but the Loss was still a little bit unstable. Lastly we incorporated BatchNormalization and a Scheduler for the learning which then gave us a loss that we found acceptable in terms of stability.
The output of the Generator was not great at that point, but it was visible that it learned something from the data. We thought maybe we had some sort of mode collapse so we modified the loss and the Discriminator to a Wasserstein GAN. The loss of the Wasserstein GAN was also very stable, but the images were sometimes almost completely black. Other than that there was no visible improvement or negative development compared to the normal GAN.
There are several things that we could still try out but were not able to, because of the lack of time. First we could try out different sizes of the latent space, maybe using a bigger latent space in combination with a higher output dimension would yield better results. We could try different patch_sizes as we only computed our results of patch_size = 64 (The patch_sizes need to be divisible by for in order for the model to work). And we could also try deeper models, as we did not improve the architecture again after stabilizing the loss. The results of different data sizes like “00” were of similar quality, one could see some sort of fiber structure, but it was very blurry. Maybe focusing on a different size from the beginning would yield better results.

To evaluate our models, we computed the Fréchet Inception Distance (FID) score using 1000 generated samples compared against real FOM patches from Vervet1947. The traditional GAN achieved an FID score of 206.76, while the WGAN scored 275.08. These relatively high scores indicate that both models struggled to fully capture the complex distribution of FOM patterns, with the traditional GAN surprisingly outperforming the WGAN despite the latter's theoretical advantages. 

Several factors likely contributed to these high FID scores. First, FOMs represent complex 3D fiber orientations through their RGB values, making them inherently more challenging to generate than typical natural images. Additionally, our relatively simple architecture might lack the capacity to capture all the nuances of fiber orientations. Despite implementing various stability measures, the models might have suffered from mode collapse or inadequate convergence during training. For future improvements, we recommend several enhancements. The architecture could benefit from progressive growing, self-attention layers, or increased network depth. Training optimizations might include experimenting with different learning rates and implementing gradient penalty for the WGAN. Additionally, the data processing pipeline could be enhanced by increasing patch size for better context or implementing specialized normalization techniques for FOM data. In conclusion, while both models demonstrate the ability to generate FOM-like patterns, the high FID scores suggest significant room for improvement. The traditional GAN showed better performance, but both approaches would benefit from architectural improvements and more sophisticated training strategies to better capture the complex nature of fiber orientation maps. Despite these challenges, our implementation provides a foundation for further research into generating these specialized scientific visualizations.

We used a Pix2Pix model for paired image-to-image translation tasks, utilizing a conditional generative adversarial network architecture. The Generator is based on a U-Net design with skip connections, which aids in preserving fine-grained spatial details while transforming input images into target outputs. The Discriminator employs a PatchGAN architecture, evaluating 70x70 pixel patches for localized realism, improving the model's efficiency and performance. The model was trained for 20 epochs using paired datasets with 256x256 image patches. Training batches contained 32 samples, while validation batches used a size of 8. The adversarial loss encouraged the generator to produce realistic images, while the L1 reconstruction loss penalized pixel-wise differences, ensuring fidelity to the ground truth. Both the generator and discriminator were optimized using the Adam optimizer with a learning rate of 0.0002 and beta values of (0.5, 0.999) for stable and consistent convergence.

|      |    MSE      | SSIM  | PSNR |
| :--: | :---------: |:-----:| :-----:|
| Mean | 0.1430      | 0.2122 | 8.4567 |
| Std  | 0.0103      | 0.0195 | 0.3045 |

The evaluation results reveal areas of improvement for our model. The moderate MSE suggests pixel-wise differences between generated and target images, but its low standard deviation indicates consistent errors. The low SSIM and PSNR values highlight significant structural and perceptual disparities between the generated images and ground truth, despite consistent performance across samples as shown by the low standard deviations. These findings suggest that while the model's performance is stable, it requires enhancements to better align the outputs with the target images. To improve the model's performance, consider strategies such as incorporating perceptual loss functions, optimizing the architecture for structural similarity or enhancing the training dataset through augmentation and preprocessing techniques.
