{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=True, use_dropout=False):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "        if downsample:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
    "        else:\n",
    "            self.conv1 = nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
    "            \n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout(0.5) if use_dropout else None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm(x)\n",
    "        if self.use_dropout and self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        return F.leaky_relu(x, 0.2) if self.downsample else F.relu(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.e1 = nn.Conv2d(1, 64, 4, 2, 1)  # 1 channel input (transmittance)\n",
    "        self.e2 = UNetBlock(64, 128)\n",
    "        self.e3 = UNetBlock(128, 256)\n",
    "        self.e4 = UNetBlock(256, 512)\n",
    "        self.e5 = UNetBlock(512, 512)\n",
    "        self.e6 = UNetBlock(512, 512)\n",
    "        self.e7 = UNetBlock(512, 512)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Conv2d(512, 512, 4, 2, 1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.d1 = UNetBlock(512, 512, False, True)\n",
    "        self.d2 = UNetBlock(1024, 512, False, True)\n",
    "        self.d3 = UNetBlock(1024, 512, False, True)\n",
    "        self.d4 = UNetBlock(1024, 512, False)\n",
    "        self.d5 = UNetBlock(1024, 256, False)\n",
    "        self.d6 = UNetBlock(512, 128, False)\n",
    "        self.d7 = UNetBlock(256, 64, False)\n",
    "        \n",
    "        self.final = nn.ConvTranspose2d(128, 3, 4, 2, 1)  # 3 channel output (FOM)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = F.leaky_relu(self.e1(x), 0.2)\n",
    "        e2 = self.e2(e1)\n",
    "        e3 = self.e3(e2)\n",
    "        e4 = self.e4(e3)\n",
    "        e5 = self.e5(e4)\n",
    "        e6 = self.e6(e5)\n",
    "        e7 = self.e7(e6)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = F.leaky_relu(self.bottleneck(e7), 0.2)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d1 = self.d1(b)\n",
    "        d2 = self.d2(torch.cat([d1, e7], 1))\n",
    "        d3 = self.d3(torch.cat([d2, e6], 1))\n",
    "        d4 = self.d4(torch.cat([d3, e5], 1))\n",
    "        d5 = self.d5(torch.cat([d4, e4], 1))\n",
    "        d6 = self.d6(torch.cat([d5, e3], 1))\n",
    "        d7 = self.d7(torch.cat([d6, e2], 1))\n",
    "        \n",
    "        out = torch.tanh(self.final(torch.cat([d7, e1], 1)))\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(4, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(512, 1, 4, 1, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        return self.model(torch.cat([x, y], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from dataloader_ex4 import create_paired_dataloader\n",
    "import os\n",
    "\n",
    "def train_pix2pix(train_loader: DataLoader, val_loader: DataLoader = None, \n",
    "                  num_epochs: int = 100, device: str = 'cuda'):\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = Generator().to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "    \n",
    "    # Loss functions\n",
    "    criterion_gan = nn.BCEWithLogitsLoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    \n",
    "    # Optimizers\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for batch in pbar:\n",
    "            real_input = batch['input'].to(device)\n",
    "            real_target = batch['target'].to(device)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            fake_output = generator(real_input)\n",
    "            fake_disc_input = torch.cat([real_input, fake_output], 1)\n",
    "            fake_disc_output = discriminator(real_input, fake_output.detach())\n",
    "            \n",
    "            real_disc_input = torch.cat([real_input, real_target], 1)\n",
    "            real_disc_output = discriminator(real_input, real_target)\n",
    "            \n",
    "            d_loss_real = criterion_gan(real_disc_output, \n",
    "                                      torch.ones_like(real_disc_output))\n",
    "            d_loss_fake = criterion_gan(fake_disc_output, \n",
    "                                      torch.zeros_like(fake_disc_output))\n",
    "            d_loss = (d_loss_real + d_loss_fake) * 0.5\n",
    "            \n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            optimizer_g.zero_grad()\n",
    "            \n",
    "            fake_output = generator(real_input)\n",
    "            fake_disc_output = discriminator(real_input, fake_output)\n",
    "            \n",
    "            g_loss_gan = criterion_gan(fake_disc_output, torch.ones_like(fake_disc_output))\n",
    "            g_loss_l1 = criterion_l1(fake_output, real_target) * 100\n",
    "            g_loss = g_loss_gan + g_loss_l1\n",
    "            \n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'D_loss': f'{d_loss.item():.4f}',\n",
    "                'G_loss': f'{g_loss.item():.4f}'\n",
    "            })\n",
    "            \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
    "                'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
    "            }, f'pix2pix_checkpoint_epoch_{epoch+1}.pth')\n",
    "            \n",
    "    return generator, discriminator\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataloaders\n",
    "    data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "    train_loader = create_paired_dataloader(\n",
    "        data_dir=data_dir,\n",
    "        brain=\"Vervet1818\",\n",
    "        patch_size=256,\n",
    "        batch_size=8,\n",
    "        tiles_per_epoch=1000\n",
    "    )\n",
    "    \n",
    "    val_loader = create_paired_dataloader(\n",
    "        data_dir=data_dir,\n",
    "        brain=\"Vervet1947\",\n",
    "        patch_size=256,\n",
    "        batch_size=8,\n",
    "        tiles_per_epoch=1000\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "train_pix2pix(train_loader, val_loader, epochs=20)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
